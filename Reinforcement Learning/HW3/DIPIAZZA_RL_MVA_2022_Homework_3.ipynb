{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# MVA - Homework 3 - Reinforcement Learning (2022/2023)\n","\n","**Name:** DI PIAZZA Théo\n","\n","**Email:**    theo.dipiazza@gmail.com\n","\n","\n","Note that the assignment is composed by two parts. This one is the practical part."],"metadata":{"id":"PklL_7_Wn6Bz"}},{"cell_type":"markdown","source":["## Instructions\n","\n","* The deadline is **January 20 (2023) at 11:59 pm (Paris time).**\n","\n","* By doing this homework you agree to the late day policy, collaboration and misconduct rules reported on [Piazza](https://piazza.com/class/l4y5ubadwj64mb/post/6).\n","\n","* **Mysterious or unsupported answers will not receive full credit**. A correct answer, unsupported by calculations, explanation, or algebraic work will receive no credit; an incorrect answer supported by substantially correct calculations and explanations might still receive partial credit.\n","\n","* Answers should be provided in **English**."],"metadata":{"id":"5n4fvDATsjPj"}},{"cell_type":"code","metadata":{"id":"WrQ-yXS-k0Pm"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from typing import Any, Optional, Dict, Tuple\n","from collections import deque, defaultdict\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import normalize as normalize_matrix\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XEL2pObSzV9Q"},"source":["# Problem\n","\n","In this exercise, we investigate the performance of LinUCB. In particular, we want to understand the impact of the representation on the learning process. \n","\n","A representation is a mapping $\\phi_i : S \\times A \\to \\mathbb{R}^{d_i}$ where $S$ is the context space and $A$ is the action space. A representation is **realizable** when $\\exists \\theta \\in \\mathbb{R}^{d_i}$ such that $r(s,a) = \\phi_i(s,a)^\\top \\theta$, for all $s,a$.\n","\n","Note that a linear contextual bandit problem admits multiple realizable representations. The question we want to investigate is: \n","\n","*Are all the representations equally good for learning?*"]},{"cell_type":"markdown","metadata":{"id":"G8mzv1xm2wyQ"},"source":["### Environment\n","We start definining utility functions and the environment we are going to use. \n","\n","In particular, `LinearEnv` defines a contextual linear bandit problem with stochastic context selection $s_t \\sim \\rho$.\n"]},{"cell_type":"code","metadata":{"id":"EFLJdp8DlWpC"},"source":["#@title Utilities for building linear representations {display-mode: \"form\"}\n","def normalize_linrep(features, param, scale=1.):\n","    param_norm = np.linalg.norm(param)\n","    new_param = param / param_norm * scale\n","    new_features = features * param_norm / scale\n","    return new_features, new_param\n","\n","def random_transform(features, param, normalize=True, seed=0):\n","    rng = np.random.RandomState(seed)\n","    dim = len(param)\n","    A = rng.normal(size=(dim, dim))\n","\n","    A = rng.normal(size=(dim, dim))\n","    q, r = np.linalg.qr(A)\n","    \n","    new_features = features @ q\n","    new_param = q.T @ param\n","        \n","    if normalize:\n","        new_features, new_param = normalize_linrep(new_features, new_param)\n","    \n","    val = features @ param - new_features @ new_param\n","    assert np.allclose(features @ param, new_features @ new_param)\n","    return new_features, new_param\n","\n","def make_random_linrep(\n","    n_contexts, n_actions, feature_dim, \n","    ortho=True, normalize=True, seed=0,\n","    method=\"gaussian\"):\n","\n","    rng = np.random.RandomState(seed)\n","    if method == \"gaussian\":\n","        features = rng.normal(size=(n_contexts, n_actions, feature_dim))\n","    elif method == \"bernoulli\":\n","        features = rng.binomial(n=1, p=rng.rand(), size=(n_contexts, n_actions, feature_dim))\n","\n","    param = 2 * rng.uniform(size=feature_dim) - 1\n","    \n","    #Orthogonalize features\n","    if ortho:\n","        features = np.reshape(features, (n_contexts * n_actions, feature_dim))\n","        orthogonalizer = PCA(n_components=feature_dim, random_state=seed) #no dimensionality reduction\n","        features = orthogonalizer.fit_transform(features)\n","        features = np.reshape(features, (n_contexts, n_actions, feature_dim))\n","        features = np.take(features, rng.permutation(feature_dim), axis=2)\n","    \n","    if normalize:\n","        features, param = normalize_linrep(features, param)\n","        \n","    return features, param\n","\n","def derank_hls(features, param, newrank=1, transform=True, normalize=True, seed=0):\n","    nc = features.shape[0]\n","\n","    rewards = features @ param\n","    # compute optimal arms\n","    opt_arms = np.argmax(rewards, axis=1)\n","    # compute features of optimal arms\n","    opt_feats = features[np.arange(nc), opt_arms, :]\n","    opt_rews = rewards[np.arange(nc), opt_arms].reshape((nc, 1)) \n","    remove = min(max(nc - newrank + 1, 0), nc)\n","    \n","    new_features = np.array(features)\n","    outer = np.matmul(opt_rews[:remove], opt_rews[:remove].T)\n","    xx = np.matmul(outer, opt_feats[:remove, :]) \\\n","        / np.linalg.norm(opt_rews[:remove])**2\n","    new_features[np.arange(remove), opt_arms[:remove], :] = xx\n","    \n","    new_param = param.copy()\n","    \n","    if transform:\n","        new_features, new_param = random_transform(new_features, new_param, normalize=normalize, seed=seed)\n","    elif normalize:\n","        new_features, new_param = normalize_linrep(new_features, new_param, seed=seed)\n","        \n","    assert np.allclose(features @ param, new_features @ new_param)\n","    return new_features, new_param\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"elthSJgamhEj","cellView":"form"},"source":["#@title classLinearEnv()\n","class LinearEnv():\n","    def __init__(self, features, param, rew_noise=0.5, random_state=0) -> None:\n","        self.features = features\n","        self.param = param\n","        self.rewards = features @ param\n","        self.rew_noise = rew_noise\n","        self.random_state = random_state\n","        self.rng = np.random.RandomState(random_state)\n","        self.n_contexts, self.n_actions, self.feat_dim = self.features.shape\n","\n","    def get_available_actions(self):\n","        \"\"\" Return the actions available at each time\n","        \"\"\"\n","        actions = np.arange(self.n_actions)\n","        return actions\n","    \n","    def sample_context(self):\n","        \"\"\" Return a random context\n","        \"\"\"\n","        self.idx = self.rng.choice(self.n_contexts, 1).item()\n","        return self.idx\n","\n","    def step(self, action):\n","        \"\"\" Return a realization of the reward in the context for the selected action\n","        \"\"\"\n","        return self.rewards[self.idx, action] + self.rng.randn() * self.rew_noise\n","\n","    def best_reward(self):\n","        \"\"\" Maximum reward in the current context\n","        \"\"\"\n","        return self.rewards[self.idx].max()\n","    \n","    def expected_reward(self, action):\n","        return self.rewards[self.idx, action]\n","\n","class LinearRepresentation():\n","    \"\"\" Returns the features associated to each context and action\n","    \"\"\"\n","    def __init__(self, features) -> None:\n","        self.features = features\n","    \n","    def features_dim(self):\n","        return self.features.shape[2]\n","    \n","    def get_features(self, context, action):\n","        return self.features[context, action]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Definition of the environment and example of interaction loop."],"metadata":{"id":"Zd8LoQzUpKkR"}},{"cell_type":"code","metadata":{"id":"FTdE7MhtlPtH"},"source":["SEED = 0\n","NOISE = 0.5\n","nc, na, dim = 100, 5, 10\n","features, param = make_random_linrep(\n","    n_contexts=nc, n_actions=na, feature_dim=dim, \n","    ortho=True, normalize=True, seed=SEED, method=\"gaussian\")\n","\n","env = LinearEnv(features=features, param=param, rew_noise=NOISE)\n","for t in range(10):\n","    context = env.sample_context()\n","    avail_actions = env.get_available_actions()\n","    # random action selection\n","    action = np.random.choice(avail_actions, 1).item()\n","    reward = env.step(action)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jp4smEDiowQZ"},"source":["# Step 1: LinUCB with different representations\n","Implement and test LinUCB with different representations"]},{"cell_type":"code","source":["#@title\n","class LinUCB:\n","  \n","    def __init__(self, \n","                 env, representation, reg_val, noise_std,\n","                 features_bound,\n","                 param_bound, delta=0.01, random_state=0):\n","        self.env = env\n","        self.rep = representation # linear representation used by LinUCB\n","        self.reg_val = reg_val\n","        self.noise_std = noise_std # noise standard deviation\n","        self.features_bound = features_bound # bound on the features (norm2(x)<=L - slides13)\n","        self.param_bound=param_bound # bound on the parameter\n","        self.delta = delta\n","        self.random_state = random_state\n","        self.rng = np.random.RandomState(random_state)\n","\n","    def run(self, horizon):\n","        instant_reward = np.zeros(horizon)\n","        best_reward = np.zeros(horizon)\n","        \n","        dim = self.rep.features_dim()\n","        # Initialize required variables\n","        #--------------------------------------------\n","        V_inverse = np.linalg.inv(self.reg_val * np.eye(dim)) # V^(-1)\n","        b = np.zeros(dim) # b\n","        theta = np.zeros(dim) # theta\n","\n","        for t in range(horizon):\n","            context = env.sample_context()\n","            avail_actions = env.get_available_actions()\n","            # Implement the optimistic action selection\n","            #--------------------------------------------\n","            # Compute Beta (slides 13, 8_contextualbandits.pdf)\n","            beta = self.noise_std * np.sqrt( dim * np.log( ( 1 + (t*self.features_bound**2)/self.reg_val )/self.delta) ) + np.sqrt(self.reg_val) * self.param_bound\n","\n","            # Compute UCB for each action (slides 14, 8_contexualbandits.pdf)\n","            ucb_list = [] # List with UCB for each action\n","            for current_action in avail_actions:\n","              # Compute UCB for current action\n","              current_ftrs = self.rep.get_features(context, current_action)\n","              current_ucb = beta*np.sqrt( np.dot(current_ftrs, np.dot(V_inverse, current_ftrs)) ) + np.dot(current_ftrs, theta)\n","              # Add computed ucb to the list\n","              ucb_list.append(current_ucb)\n","\n","            # Select action\n","            action = avail_actions[np.argmax(ucb_list)]\n","\n","            # execute action\n","            reward = env.step(action)\n","\n","            # get features corresponding to the selected action\n","            v = self.rep.get_features(context, action)\n","\n","            # update internal model\n","            #--------------------------------------------\n","            # Update matrix A (V in slides) with Sherman-Morrison formula (slides 14, 8_contexualbandits.pdf)\n","            V_inverse -= (V_inverse @ np.outer(v, v) @ V_inverse) / (1 + v.T @ V_inverse @ v)\n","            # Update vector b (slides 12, 8_contexualbandits)\n","            b += reward * v\n","            # Update theta (slides 12, 8_contexualbandits)\n","            theta = V_inverse @ b\n","\n","            # regret computation\n","            instant_reward[t] = self.env.expected_reward(action)\n","            best_reward[t] = self.env.best_reward()\n","        \n","        # define the regret\n","        #--------------------------------------------\n","        regret = np.cumsum(best_reward - instant_reward) # slides 4\n","        #--------------------------------------------\n","        return {\"regret\": regret}\n"],"metadata":{"id":"dS1DHeboLAS1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Test the algorithm"],"metadata":{"id":"CyWRzqWE5QFO"}},{"cell_type":"code","source":["SEED = 0\n","NOISE = 0.5\n","nc, na, dim = 100, 5, 10\n","features, param = make_random_linrep(\n","    n_contexts=nc, n_actions=na, feature_dim=dim, \n","    ortho=True, normalize=True, seed=SEED, method=\"gaussian\")\n","\n","env = LinearEnv(features=features, param=param, rew_noise=NOISE)"],"metadata":{"id":"TGq2Cu6W5WmI"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rN5QgzhZq32w"},"source":["T=10000\n","NRUNS = 2\n","rep = LinearRepresentation(env.features)\n","\n","regrets = np.zeros((NRUNS,T))\n","for r in range(NRUNS):\n","    algo = LinUCB(\n","        env, representation=rep, reg_val=1, \n","        noise_std=NOISE, \n","        features_bound=np.linalg.norm(env.features,2, axis=-1).max(), \n","        param_bound=np.linalg.norm(env.param,2)\n","    )\n","    output = algo.run(T)\n","    regrets[r] = output['regret']\n","\n","mr = np.mean(regrets, axis=0)\n","vr = np.std(regrets, axis=0) / np.sqrt(NRUNS)\n","plt.plot(np.arange(T), mr)\n","plt.fill_between(np.arange(T), mr - 2*vr, mr + 2*vr, alpha=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can construct equivalent representations with the *same size*. \n","\n","We already provide the code for building such representations."],"metadata":{"id":"KR78xckV5Z5E"}},{"cell_type":"code","source":["rep_list = []\n","param_list = []\n","for i in range(1, dim):\n","    fi, pi = derank_hls(features=features, param=param, newrank=i, transform=True, normalize=True, seed=np.random.randint(1, 1234144,1))\n","    rep_list.append(LinearRepresentation(fi))\n","    param_list.append(pi)\n","rep_list.append(LinearRepresentation(features))\n","param_list.append(param)\n","\n","for i in range(len(rep_list)):\n","    print()\n","    print(f\"feature norm({i}): {np.linalg.norm(rep_list[i].features,2,axis=-1).max()}\")\n","    print(f\"param norm({i}): {np.linalg.norm(param_list[i],2)}\")\n","    assert np.allclose(rep_list[i].features @ param_list[i], features @ param) #check that they are all equivalent\n","print()"],"metadata":{"id":"fmOY0ZTO5aKT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's run LinUCB with each representation"],"metadata":{"id":"DIsp8bQAq_Ce"}},{"cell_type":"code","source":["T=20000\n","NRUNS = 2\n","\n","results = {}\n","for nr, rep in enumerate(rep_list):\n","    regrets = np.zeros((NRUNS,T))\n","    for r in range(NRUNS):\n","        algo = LinUCB(env, representation=rep, reg_val=1, noise_std=NOISE, \n","                      features_bound=np.linalg.norm(rep.features,2, axis=-1).max(),\n","                      param_bound=np.linalg.norm(param_list[nr],2)\n","                      )\n","        output = algo.run(T)\n","        regrets[r] = output['regret']\n","\n","    mr = np.mean(regrets, axis=0)\n","    vr = np.std(regrets, axis=0) / np.sqrt(NRUNS)\n","    results[f'LinUCB-rep_{nr}'] = {'regret': mr, \"std\": vr}"],"metadata":{"id":"GCt-zyC35u4Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(15,8))\n","for k,v in results.items():\n","    plt.plot(np.arange(T), v['regret'], label=k)\n","    plt.fill_between(np.arange(T), v['regret'] - 2*v['std'], v['regret'] + 2*v['std'], alpha=0.2)\n","plt.legend()"],"metadata":{"id":"kRpR5TPB8kof"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If everything is implemented correctly, there is a representation with a much better regret. \n","\n","**Q1**: Why? What is the property of such a representation?\n","\n","See [Leveraging Good Representations in Linear Contextual Bandits](https://arxiv.org/abs/2104.03781) for the answer."],"metadata":{"id":"3iuRWV_16Lk2"}},{"cell_type":"markdown","source":["**My answer to Q1**\n","\n","To start, it's important to nice that a representation is an encoding of the features of the state or context in which an action is to be taken.\n","\n","The paper explains that the contextual bandit problem can have multiple linear representations, with a different regret for each representation.\n","Moreover some representations are better than others. It's the case when a constant problem-dependent regret can be achieved.\n","\n","In our work, the best representation which corresponds to this case is the one called **\"LinUCB-rep_9\"**, whose regret is constant.\n","\n","To obtain a constant regret for LinUCB, the **HLS condition** must be satisfied which is (Figure 2 from the paper):\n","\n","$$\\lambda_{min}(\\mathbb{E}_{x \\sim \\rho} [\\phi^*(x).\\phi^{*T}(x)]) > 0$$\n","\n","Finally, this conditon is created with the *derank_hls* function."],"metadata":{"id":"fcfhssc364q2"}},{"cell_type":"markdown","metadata":{"id":"HEKKBmV1twEW"},"source":["# Step 2: representation selection"]},{"cell_type":"markdown","source":["Now that we have seen that not all the representations are equal, we want to design an algorithm able to leverage the most efficient representation when provided with a set of **realizable** representations.\n","\n","\n","This algorithm exists and is called LEADER. Implement the LEADER algorithm as reported in the paper [\"Leveraging Good Representations in Linear Contextual Bandits\"](https://arxiv.org/abs/2104.03781)."],"metadata":{"id":"EQM_QsHh6881"}},{"cell_type":"code","metadata":{"id":"wLKGjbGprSKa"},"source":["class LEADER:\n","  \n","    def __init__(self, \n","                 env, representations, reg_val, noise_std,\n","                 features_bounds,\n","                 param_bounds, delta=0.01, random_state=0\n","        ):\n","        self.env = env\n","        self.reps = representations #list of representations\n","        self.reg_val = reg_val\n","        self.noise_std = noise_std\n","        self.features_bound = features_bounds #list of feature bounds\n","        self.param_bound=param_bounds #list of parameter bounds\n","        self.delta = delta\n","        self.random_state = random_state\n","        self.rng = np.random.RandomState(random_state)\n","\n","    def run(self, horizon):\n","        instant_reward = np.zeros(horizon)\n","        best_reward = np.zeros(horizon)\n","        M = len(self.reps)\n","        \n","        inv_A = []\n","        b_vec = []\n","        A_logdet = []\n","        theta = []\n","        for i in range(M):\n","            dim = self.reps[i].features_dim()\n","            \n","            # Initialize required variables\n","            #--------------------------------------------\n","            # Initialize b_vec with zeros\n","            b_vec.append(np.zeros(dim)) # b\n","            # Initialize thetas with zeros\n","            theta.append(np.zeros(dim)) # theta\n","            # Initialize A\n","            A = self.reg_val*np.eye(dim)\n","            inv_A.append(np.linalg.inv(A)) # compute A^(-1)\n","            A_logdet.append(np.log(np.linalg.det(A)))\n","            \n","        for t in range(horizon):\n","            context = env.sample_context()\n","            avail_actions = env.get_available_actions()\n","\n","            # Implement the action selection strategy\n","            #--------------------------------------------\n","            # TODO\n","            # Iterate over number of reps\n","            ucb_all = []\n","            for i in range(M):\n","              ucb_i = []\n","              # Compute beta\n","              beta = np.sqrt(self.reg_val) * self.param_bound[i] + self.noise_std * np.sqrt(A_logdet[i] - np.log(np.linalg.det(self.reg_val * np.eye(dim))) - 2 * np.log(self.delta)) \n","              \n","              # Compute UCB for each available action\n","              actions = self.reps[i].get_features(context, avail_actions)\n","              # Iterate over available actions\n","              for current_action in actions:\n","                # Compute UCB and add it to the current list\n","                current_ucb = beta*np.sqrt(current_action.T @ inv_A[i] @ current_action) + current_action@theta[i]\n","                ucb_i.append(current_ucb)\n","\n","              # Once UCB(s) computed at iteration i\n","              ucb_all.append(ucb_i) # Add it to the list with all UCB\n","            \n","            # Select the action which maximizes UCB over i\n","            action = np.argmax(np.min(ucb_all, axis = 0))\n","            \n","            #execute action\n","            reward = env.step(action)\n","\n","            # update\n","            for j in range(M):\n","              v = self.reps[j].get_features(context, action)\n","              # update internal model\n","              #--------------------------------------------\n","              # Update inv_A with Sherman-Morrison formula (slides 14, 8_contexualbandits.pdf)\n","              inv_A[j] -= (inv_A[j] @ np.outer(v, v) @ inv_A[j]) / (1 + v.T @ inv_A[j] @ v)\n","              # Update current A_logdet as well with current inv_A\n","              A_logdet[j] = np.log(1/np.linalg.det(inv_A[j]))\n","              # Update b and and theta\n","              b_vec[j] += reward * v\n","              theta[j] = inv_A[j] @ b_vec[j]\n","\n","            # regret computation\n","            instant_reward[t] = self.env.expected_reward(action)\n","            best_reward[t] = self.env.best_reward()\n","        \n","        \n","        # define the regret\n","        #--------------------------------------------\n","        regret = np.cumsum(best_reward - instant_reward)\n","        #--------------------------------------------\n","        return {\"regret\": regret}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p5YAbauzt3Ho"},"source":["regrets = np.zeros((NRUNS,T))\n","M = len(rep_list)\n","for r in range(NRUNS):\n","    algo = LEADER(env, representations=rep_list, reg_val=1, noise_std=NOISE, \n","                  features_bounds=[np.linalg.norm(rep_list[j].features,2, axis=-1).max() for j in range(M)], \n","                  param_bounds=[np.linalg.norm(param_list[j],2) for j in range(M)]\n","    )\n","    output = algo.run(T)\n","    regrets[r] = output['regret']\n","\n","mr = np.mean(regrets, axis=0)\n","vr = np.std(regrets, axis=0) / np.sqrt(NRUNS)\n","results['LEADER'] = {'regret': mr, 'std': vr}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LwtWqSUbvv4t"},"source":["plt.figure(figsize=(15,8))\n","for k,v in results.items():\n","    if k == 'LEADER':\n","      plt.plot(np.arange(T), v['regret'], '--', label=k)\n","    else:\n","      plt.plot(np.arange(T), v['regret'], label=k)\n","    plt.fill_between(np.arange(T), v['regret'] - 2*v['std'], v['regret'] + 2*v['std'], alpha=0.2)\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If correctly implemented, LEADER should have the second best performance\n","\n","**Q2**: so far we have considered only *realizable* representations, i.e., $\\forall i, \\exists \\theta \\in \\mathbb{R}^{d_i}$ such that $r(s,a) = \\phi_i(s,a)^\\top \\theta$, for all $s,a$. Now suppose that this property holds only for a single representation $i^\\star$, while for all $i\\neq i^\\star$ we have $\\forall \\theta \\in \\mathbb{R}^{d_i} \\exists s,a$ such that $r(s,a) \\neq \\phi_i(s,a)^\\top \\theta$. Do you think the LEADER algorithm would still work (i.e., achieve sub-linear regret) for this setting? Why?"],"metadata":{"id":"0k8-5kMW8vjW"}},{"cell_type":"markdown","source":["**Answer of Q2**\n","\n","In the LEADER Algorithm, it is assumed that all representations are realizable.\n","\n","Hence, it would not still work since the creation of the Upper Confidence Bounds should not work properly.\n","\n","The quantity to minimize is the following regret :    \n","\n","$$R_n = \\mathbb{E} [\\sum_{t=1}^n \\max_{s,a \\in \\mathcal{X}_t} r(s,a) - \\sum_{t=1}^n \\phi(s_t, a_t)^T \\theta^{*}]$$\n","\n","Hence, if we have that :\n","\n","$$r(s,a) = \\phi_{i^*}(s,a)^\\top \\theta$$\n","\n","$$\\forall i\\neq i^\\star \\mbox{we have that } \\forall \\theta \\in \\mathbb{R}^{d_i} \\exists s,a \\mbox{ such that } r(s,a) \\neq \\phi_i(s,a)^\\top \\theta$$\n","\n","It comes that a lower bound can be found on $R_n$, and the best context will not be always found so it won't work properly."],"metadata":{"id":"-4hws5Kn8zlX"}},{"cell_type":"markdown","source":["___\n","\n","## End of Homework 3.\n","\n","### Thank you for reading!\n","\n","Théo Di Piazza"],"metadata":{"id":"R2aG5HywDB5I"}},{"cell_type":"code","source":["# Connect to google colab to save the script as HTML as the end of the execution\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"u8Qf7AMmvpBg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Command to save the file as HTML\n","! jupyter nbconvert --to html /content/drive/MyDrive/ENS_MVA/Reinforcement_Learning/HW3/DIPIAZZA_RL_MVA_2022_Homework_3.ipynb"],"metadata":{"id":"9q06h-YVvvJe"},"execution_count":null,"outputs":[]}]}