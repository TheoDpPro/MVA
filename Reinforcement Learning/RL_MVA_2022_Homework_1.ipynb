{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"IAE8kMxe6E6k"},"source":["# MVA - Homework 1 - Reinforcement Learning (2022/2023)\n","\n","**Name:** DI PIAZZA Théo"]},{"cell_type":"markdown","metadata":{"id":"vY4MH0nU637o"},"source":["## Instructions\n","\n","* The deadline is **November 10 at 11:59 pm (Paris time).**\n","\n","* By doing this homework you agree to the late day policy, collaboration and misconduct rules reported on [Piazza](https://piazza.com/class/l4y5ubadwj64mb/post/6).\n","\n","* **Mysterious or unsupported answers will not receive full credit**. A correct answer, unsupported by calculations, explanation, or algebraic work will receive no credit; an incorrect answer supported by substantially correct calculations and explanations might still receive partial credit.\n","\n","* Answers should be provided in **English**."]},{"cell_type":"markdown","metadata":{"id":"YB__2uUC5U1r"},"source":["# Colab setup"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"OwHZ3a1bkA7m"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2XNj1_VZ2FGJ"},"source":["from IPython import get_ipython\n","\n","if 'google.colab' in str(get_ipython()):\n","  # install rlberry library\n","  !pip install git+https://github.com/rlberry-py/rlberry.git@mva2021#egg=rlberry[default] > /dev/null 2>&1\n","\n","  # install ffmpeg-python for saving videos\n","  !pip install ffmpeg-python > /dev/null 2>&1\n","\n","  # packages required to show video\n","  !pip install pyvirtualdisplay > /dev/null 2>&1\n","  !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n","\n","  print(\"Libraries installed, please restart the runtime!\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s8F7RiPXjutB"},"source":["# Create directory for saving videos\n","!mkdir videos > /dev/null 2>&1\n","\n","# Initialize display and import function to show videos\n","import rlberry.colab_utils.display_setup\n","from rlberry.colab_utils.display_setup import show_video"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KISV44N_nCNm"},"source":["# Useful libraries\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L4hKBRTCh6Gk"},"source":["# Preparation\n","\n","In the coding exercises, you will use a *grid-world* MDP, which is represented in Python using the interface provided by the [Gym](https://gym.openai.com/) library. The cells below show how to interact with this MDP and how to visualize it.\n"]},{"cell_type":"code","metadata":{"id":"514mHDeQooKa"},"source":["from rlberry.envs import GridWorld\n","\n","def get_env():\n","  \"\"\"Creates an instance of a grid-world MDP.\"\"\"\n","  env = GridWorld(\n","      nrows=5,\n","      ncols=7,\n","      reward_at = {(0, 6):1.0},\n","      walls=((0, 4), (1, 4), (2, 4), (3, 4)),\n","      success_probability=0.9,\n","      terminal_states=((0, 6),)\n","  )\n","  return env\n","\n","def render_policy(env, policy=None, horizon=50):\n","  \"\"\"Visualize a policy in an environment\n","\n","  Args:\n","    env: GridWorld\n","        environment where to run the policy\n","    policy: np.array\n","        matrix mapping states to action (Ns).\n","        If None, runs random policy.\n","    horizon: int\n","        maximum number of timesteps in the environment.\n","  \"\"\"\n","  env.enable_rendering()\n","  state = env.reset()                       # get initial state\n","  for timestep in range(horizon):\n","      if policy is None:\n","        action = env.action_space.sample()  # take random actions\n","      else:\n","        action = policy[state]\n","      next_state, reward, is_terminal, info = env.step(action)\n","      state = next_state\n","      if is_terminal:\n","        break\n","  # save video and clear buffer\n","  env.save_video('./videos/gw.mp4', framerate=5)\n","  env.clear_render_buffer()\n","  env.disable_rendering()\n","  # show video\n","  show_video('./videos/gw.mp4')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hQAHUBw_ifMI"},"source":["# Create an environment and visualize it\n","env = get_env()\n","render_policy(env)  # visualize random policy\n","\n","# The reward function and transition probabilities can be accessed through\n","# the R and P attributes:\n","print(f\"Shape of the reward array = (S, A) = {env.R.shape}\")\n","print(f\"Shape of the transition array = (S, A, S) = {env.P.shape}\")\n","print(f\"Reward at (s, a) = (1, 0): {env.R[1, 0]}\")\n","print(f\"Prob[s\\'=2 | s=1, a=0]: {env.P[1, 0, 2]}\")\n","print(f\"Number of states and actions: {env.Ns}, {env.Na}\")\n","\n","# The states in the griworld correspond to (row, col) coordinates.\n","# The environment provides a mapping between (row, col) and the index of\n","# each state:\n","print(f\"Index of state (1, 0): {env.coord2index[(1, 0)]}\")\n","print(f\"Coordinates of state 5: {env.index2coord[5]}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ibGD_3I89CNu"},"source":["# Part 1 - Dynamic Programming"]},{"cell_type":"markdown","metadata":{"id":"NR7h5won9NQY"},"source":["## Question 1.1\n","\n","Consider a general MDP with a discount factor of $\\gamma < 1$. Assume that the horizon is infinite (so there is no termination). A policy $\\pi$ in this MDP\n","induces a value function $V^\\pi$. Suppose an affine transformation is applied to the reward, what is\n","the new value function? Is the optimal policy preserved?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"313W4K3B_LtN"},"source":["### **Answer**\n","\n","To start, let's consider :\n","- A policy $\\pi = (d_t)_{t\\geq0}$, which induces a value faction $V_\\pi$.\n","- $\\forall (s,a) \\in (S, A)$, the new reward $r_{aff}(s,a)$, obtained after an affine transformation of the original reward $r(s,a)$ such that : $r_{aff}(s,a) = A*r(s,a)+B$\n","\n","<br>\n","\n","We know by definition that :     \n","$$\\forall s \\in S, \\ V^{\\pi}(s) =  \\mathbb{E}[\\sum_{t = 0}^{\\infty} \\gamma^{t} . r(s_t, d_t(s_t)) \\ \\vert \\ s_0=s, \\ \\pi] $$\n","\n","<br>\n","\n","Then, if we define $V^{\\pi}_{aff}$ the new value function obtained after an affine transformation applied to the reward :\n","\n","\n","$$\\forall s \\in S, \\ V^{\\pi}_{aff}(s) =  \\mathbb{E}[\\sum_{t = 0}^{\\infty} \\gamma^{t} . r_{affine}(s_t, d_t(s_t)) \\ \\vert \\ s_0=s, \\ \\pi] = \n","\\mathbb{E}[\\sum_{t = 0}^{\\infty} \\gamma^{t} . A*r(s_t, d_t(s_t))+B \\ \\vert \\ s_0=s, \\ \\pi] = A \\ \\mathbb{E}[\\sum_{t = 0}^{\\infty} \\gamma^{t} . r(s_t, d_t(s_t)) \\ \\vert \\ s_0=s, \\ \\pi] + B\\sum_{t = 0}^{\\infty} \\gamma^{t} = A \\ V^{\\pi}(s)+\\frac{B}{1-γ}$$\n","\n","\n","<br>\n","\n","Finally, if we consider $\\pi^{*}$ the optimal policy which induces the value function $V^{\\pi^{*}}$, we have that for any policy $\\pi$ :\n","\n","\n","$$V^{\\pi^*}_{aff}(s) \\geq V^{\\pi}_{aff} \\iff A \\ V^{\\pi^*}(s)+\\frac{B}{1-γ} = A \\ V^{\\pi}(s)+\\frac{B}{1-γ} \\iff A \\ (V^{\\pi^*}(s)-V^{\\pi}(s)) \\geq 0 \\iff A \\geq 0$$\n","\n","<br>\n","\n","Hence, the optimal policy is preserved if and only if $A \\geq 0$."]},{"cell_type":"markdown","metadata":{"id":"0uCVgkDo9vTM"},"source":["## Question 1.2\n","\n","Consider an infinite-horizon $\\gamma$-discounted MDP. We denote by $Q^*$ the $Q$-function of the optimal policy $\\pi^*$. Prove that, for any function $Q(s, a)$ (which is **not** necessarily the value function of a policy), the following inequality holds for any state $s$:\n","\n","$$\n","V^{\\pi_Q}(s) \\geq V^*(s) - \\frac{2}{1-\\gamma}||Q^*-Q||_\\infty,\n","$$\n","\n","where $||Q^*-Q||_\\infty = \\max_{s, a} |Q^*(s, a) - Q(s, a)|$ and $\\pi_Q(s) \\in \\arg\\max_a Q(s, a)$. Can you use this result to show that any policy $\\pi$ such that $\\pi(s) \\in \\arg\\max_a Q^*(s, a)$ is optimal?"]},{"cell_type":"markdown","metadata":{"id":"8MqGWPPD_OAI"},"source":["### **Answer**\n","\n","To start, it's recalled that :\n","\n","- $\\forall s \\in S, \\ \\mathcal{T}^{\\pi_Q}V^{*}(s) = r(s,\\pi_Q(s)) + \\gamma \\sum_{s'} p(s' \\vert s,\\pi_Q(s))V^{*}(s') = r(s,\\pi_Q(s)) + \\gamma \\sum_{s'} p(s' \\vert s,\\pi_Q(s))Q^{*}(s', \\pi_Q(s')) = Q^{*}(s, \\pi_Q(s))$\n","- $\\forall s \\in S, \\ \\mathcal{T}^{\\pi_Q}V^{\\pi_Q}(s) = V^{\\pi_Q}(s)$\n","\n","<br>\n","\n","Then it can be showned that : \n","\n","$\\forall (s, a) \\in S.A$ : \n","\n","$$| V^{\\pi_Q}(s) - V^{*}(s) | = | V^{\\pi_Q}(s) - Q^{*}(s, \\pi_Q(s)) + Q^{*}(s, \\pi_Q(s)) - Q(s, a) + Q(s, a) - V^{*}(s) |$$\n","\n","$$ \\leq | V^{\\pi_Q}(s) - Q^{*}(s, \\pi_Q(s)) | + | Q^{*}(s, \\pi_Q(s)) - Q(s, a) | + | Q(s, a) - V^{*}(s) |$$\n","\n","<br>\n","\n","Where :\n","\n","- $| V^{\\pi_Q}(s) - Q^{*}(s, \\pi_Q(s)) | = | \\mathcal{T}^{\\pi_Q}V^{\\pi_Q}(s) - \\mathcal{T}^{\\pi_Q}V^{*}(s) | \\leq γ||V^{\\pi_Q}-V^{*}||_{\\infty}$\n","- $| Q^{*}(s, \\pi_Q(s)) - Q(s, a) | \\leq \\max_{a, s} | Q^{*}(s, a) - Q(s, a) | = ||Q^{*}-Q||_{\\infty}$\n","- $| Q(s, a) - V^{*}(s) | = | Q(s, a) -  Q^{*}(s, \\pi_Q(s)) | \\leq ||Q^{*}-Q||_{\\infty}$\n","\n","<br>\n","\n","Hence, finally : \n","\n","$$ ||V^{\\pi_Q}-V^{*}||_{\\infty} \\leq \\gamma||V^{\\pi_Q}-V^{*}||_{\\infty}+ 2||Q^{*}-Q||_{\\infty} \\iff ||V^{\\pi_Q}-V^{*}||_{\\infty} \\leq \\frac{2}{1-\\gamma}||Q^{*}-Q||_{\\infty}$$\n","\n","<br>\n","\n","Then since $V^{*}$ is the value function of the optimal policy, it can be written that $\\forall s \\in S$ : \n","\n","$$ V^{\\pi_Q}(s)-V^{*}(s) \\geq -\\frac{2}{1-\\gamma}||Q^{*}-Q||_{\\infty}$$"]},{"cell_type":"markdown","metadata":{"id":"yIrtb7sihYcM"},"source":["## Question 1.3\n","\n","In this question, you will implement and compare the policy and value iteration algorithms for a finite MDP. \n","\n","Complete the functions `policy_evaluation`, `policy_iteration` and `value_iteration` below.\n","\n","\n","Compare value iteration and policy iteration. Highlight pros and cons of each method."]},{"cell_type":"markdown","metadata":{"id":"GLmQtk-wt0HS"},"source":["### **Answer**\n","\n","Pros and cons of each method :\n","\n","<br>\n","\n","#### **Policy Iteration**:\n","\n","**Pros**: Converge in a finite number of iterations.\n","\n","**Cons**:  each iteration requires a full policy evaluation and it might be expensive.\n","\n","<br>\n","\n","##### **Value Iteration**:\n","\n","**Pros**: each iteration is very computationally efficient.\n","\n","**Cons**: convergence is only asymptotic.\n","\n"]},{"cell_type":"code","metadata":{"id":"9yI0YYtMmpDQ"},"source":["# Link : http://incompleteideas.net/book/ebook/node41.html\n","def policy_evaluation(P, R, policy, gamma=0.9, tol=1e-2):\n","    \"\"\"\n","    Args:\n","        P: np.array\n","            transition matrix (NsxNaxNs)\n","        R: np.array\n","            reward matrix (NsxNa)\n","        policy: np.array\n","            matrix mapping states to action (Ns)\n","        gamma: float\n","            discount factor\n","        tol: float\n","            precision of the solution\n","    Return:\n","        value_function: np.array\n","            The value function of the given policy\n","    \"\"\"\n","    # Number of states, actions\n","    Ns, Na = R.shape\n","\n","    # ====================================================\n","\t  # YOUR IMPLEMENTATION HERE \n","    #\n","    value_function = np.zeros(Ns)\n","\n","    # 2 possibles computation : one with precision criterium, and one without\n","    \n","    # [1/2] : Direct computation without precision criterium\n","    if(tol==0):\n","\n","      # Identity matrix (Ns.Ns)\n","      matrix_I = np.eye(Ns)\n","\n","      # Compute matrix P and R \n","      P_direct, R_direct = np.zeros([Ns, Ns]), np.zeros(Ns)\n","\n","      # Fill these matrixes\n","      for s in range(Ns):\n","        P_direct[s] = P[s, policy[s], :]\n","        R_direct[s] = R[s, policy[s]]\n","\n","      # Compute value function\n","      value_function = np.linalg.inv(matrix_I - gamma*P_direct) @ R_direct # Matmul product\n","\n","      return value_function\n","\n","    # [2/2] : Iterative computation with precision criterium\n","    elif(tol > 0):\n","\n","      criterium = False\n","\n","      while(not criterium):\n","\n","        current_value_function = value_function.copy()\n","\n","        for s in range(Ns):\n","          value_function[s] = R[s,policy[s]] + gamma*P[s,policy[s],:]@value_function\n","\n","        # Update criterium and value function\n","        criterium = ( max(abs(current_value_function - value_function)) < tol )\n","\n","      return value_function\n","    # ===================================================="],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ncqbPx99ncVY"},"source":["\"\"\"\n","  The policy iteration algorithm generates a sequences of policies with non-decreasing\n","  performance. And it converges to the optimal policy in a finite number of iteraties.\n","\"\"\"\n","\n","def policy_iteration(P, R, gamma=0.9, tol=1e-3):\n","    \"\"\"\n","    Args:\n","        P: np.array\n","            transition matrix (NsxNaxNs)\n","        R: np.array\n","            reward matrix (NsxNa)\n","        gamma: float\n","            discount factor\n","        tol: float\n","            precision of the solution\n","    Return:\n","        policy: np.array\n","            the final policy\n","        V: np.array\n","            the value function associated to the final policy\n","    \"\"\"\n","    Ns, Na = R.shape\n","    V = np.zeros(Ns)\n","    policy = np.ones(Ns, dtype=int)\n","    # ====================================================\n","\t  # YOUR IMPLEMENTATION HERE \n","    #\n","\n","    stop_criterium = False\n","\n","    # At each iteration\n","    while(not stop_criterium):\n","\n","      V_policy = V.copy()\n","\n","      # Policy evaluation : given policy, compute value function\n","      V = policy_evaluation(P, R, policy, gamma, tol)\n","\n","      # Policy improvement : compute the greedy policy\n","      policy = np.argmax(R+gamma*(P@V), axis=1)\n","\n","      stop_criterium = ((V == V_policy).all())\n","\n","    # ====================================================\n","    return policy, V"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3jB7dfA5nRCZ"},"source":["\"\"\"\n","  Value iteration is a method of computing an optimal MDP policy and its value.\n","\"\"\"\n","\n","def value_iteration(P, R, gamma=0.9, tol=1e-3):\n","    \"\"\"\n","    Args:\n","        P: np.array\n","            transition matrix (NsxNaxNs)\n","        R: np.array\n","            reward matrix (NsxNa)\n","        gamma: float\n","            discount factor\n","        tol: float\n","            precision of the solution\n","    Return:\n","        Q: final Q-function (at iteration n)\n","        greedy_policy: greedy policy wrt Qn\n","        Qfs: all Q-functions generated by the algorithm (for visualization)\n","    \"\"\"\n","    Ns, Na = R.shape\n","    Q = np.zeros((Ns, Na))\n","    Qfs = [Q]\n","    # ====================================================\n","\t  # YOUR IMPLEMENTATION HERE \n","    #\n","\n","    stop_criterium = False # Criterium to stop iteration\n","    V = np.zeros(Ns) # Value function associated to Q-function\n","\n","    while(not stop_criterium):\n","\n","      # value contains value function of previous iteration\n","      value = V.copy()\n","\n","      # Compute Q-function of current iteration\n","      Q = R+gamma*(P@V)\n","      # Stock it for visualization\n","      Qfs.append(Q)\n","      # Compute the greedy policy of current iteration\n","      greedy_policy = np.argmax(Q, axis=1)\n","\n","      # Compute new value function associated to the current Q-function, and update stop criterium\n","      V = np.max(Q, axis = 1)\n","      stop_criterium = ( max(abs(V-value)) < tol )\n","\n","    # ====================================================\n","    return Q, greedy_policy, Qfs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Fi0IzZJp74Z"},"source":["### Testing your code"]},{"cell_type":"code","metadata":{"id":"P7JKrc1oqFI2"},"source":["# Parameters\n","tol = 1e-5\n","gamma = 0.99\n","\n","# Environment\n","env = get_env()\n","\n","# run value iteration to obtain Q-values\n","VI_Q, VI_greedypol, all_qfunctions = value_iteration(env.P, env.R, gamma=gamma, tol=tol)\n","\n","# render the policy\n","print(\"[VI]Greedy policy: \")\n","render_policy(env, VI_greedypol)\n","\n","# compute the value function of the greedy policy using matrix inversion\n","# ====================================================\n","# YOUR IMPLEMENTATION HERE \n","# compute value function of the greedy policy\n","#\n","\n","greedy_V = policy_evaluation(env.P, env.R, VI_greedypol, gamma, tol)\n","\n","# ====================================================\n","\n","# show the error between the computed V-functions and the final V-function\n","# (that should be the optimal one, if correctly implemented)\n","# as a function of time\n","final_V = all_qfunctions[-1].max(axis=1)\n","norms = [ np.linalg.norm(q.max(axis=1) - final_V) for q in all_qfunctions]\n","plt.plot(norms)\n","plt.xlabel('Iteration')\n","plt.ylabel('Error')\n","plt.title(\"Value iteration: convergence\")\n","\n","#### POLICY ITERATION ####\n","PI_policy, PI_V = policy_iteration(env.P, env.R, gamma=gamma, tol=tol)\n","print(\"\\n[PI]final policy: \")\n","render_policy(env, PI_policy)\n","\n","## Uncomment below to check that everything is correct\n","assert np.allclose(PI_policy, VI_greedypol),\\\n","    \"You should check the code, the greedy policy computed by VI is not equal to the solution of PI\"\n","assert np.allclose(PI_V, greedy_V),\\\n","    \"Since the policies are equal, even the value function should be\"\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2V1QdoH-xFX0"},"source":["# Part 2 - Tabular RL"]},{"cell_type":"markdown","metadata":{"id":"Qf51VhoPxbV4"},"source":["## Question 2.1\n","\n","The code below collects two datasets of transitions (containing states, actions, rewards and next states) for a discrete MDP.\n","\n","For each of the datasets:\n","\n","1. Estimate the transitions and rewards, $\\hat{P}$ and $\\hat{R}$.\n","2. Compute the optimal value function and the optimal policy with respect to the estimated MDP (defined by $\\hat{P}$ and $\\hat{R}$), which we denote by $\\hat{\\pi}$ and $\\hat{V}$.\n","3. Numerically compare the performance of $\\hat{\\pi}$ and $\\pi^\\star$ (the true optimal policy), and the error between $\\hat{V}$ and $V^*$ (the true optimal value function).\n","\n","Which of the two data collection methods do you think is better? Why?"]},{"cell_type":"markdown","metadata":{"id":"eWSyewG2EZpJ"},"source":["### **Answer**\n","\n","$\\Longrightarrow$ Concerning the comparison of the **performances of the different policies**, we notice that the policy obtained with dataset 2 (*uniform sampling*) evolves almost all the time towards the optimal solution. On the other hand, the policy obtained with dataset 1 (*random sampling*) does not evolve towards the solution, especially when the number of samples is too small.\n","\n","<br>\n","\n","$\\Longrightarrow$ Concerning the comparison of the error between the **estimated and exact function values**, it can be seen that the errors are much smaller (both L1 and L-Inf norm) for the function value estimated by dataset 2 (*uniform sampling*) than dataset 1 (*random sampling*).\n","\n","<br>\n","\n","$\\Longrightarrow$ For these reasons, the method that seems to be the most robust and efficient for this kind of problem is **method 2 (by uniformly sampling states and actions)**."]},{"cell_type":"code","metadata":{"id":"8lNPhB28EcGd"},"source":["def get_random_policy_dataset(env, n_samples):\n","  \"\"\"Get a dataset following a random policy to collect data.\"\"\"\n","  states = []\n","  actions = []\n","  rewards = []\n","  next_states = []\n","  \n","  state = env.reset()\n","  for _ in range(n_samples):\n","    action = env.action_space.sample()\n","    next_state, reward, is_terminal, info = env.step(action)\n","    states.append(state)\n","    actions.append(action)\n","    rewards.append(reward)\n","    next_states.append(next_state)\n","    # update state\n","    state = next_state\n","    if is_terminal:\n","      state = env.reset()\n","\n","  dataset = (states, actions, rewards, next_states)\n","  return dataset\n","\n","def get_uniform_dataset(env, n_samples):\n","  \"\"\"Get a dataset by uniformly sampling states and actions.\"\"\"\n","  states = []\n","  actions = []\n","  rewards = []\n","  next_states = []\n","  for _ in range(n_samples):\n","    state = env.observation_space.sample()\n","    action = env.action_space.sample()\n","    next_state, reward, is_terminal, info = env.sample(state, action)\n","    states.append(state)\n","    actions.append(action)\n","    rewards.append(reward)\n","    next_states.append(next_state)\n","\n","  dataset = (states, actions, rewards, next_states)\n","  return dataset\n","\n","\n","# Collect two different datasets\n","num_samples = 1000\n","env = get_env()\n","dataset_1 = get_random_policy_dataset(env, num_samples)\n","dataset_2 = get_uniform_dataset(env, num_samples)\n","\n","\n","# Item 3: Estimate the MDP with the two datasets; compare the optimal value\n","# functions in the true and in the estimated MDPs\n","\n","# ..."],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To start, we define a function getMatrixPR which returns P_hat and R_hat, for a given environment and dataset.\n","\n","def getMatrixPR(env, dataset):\n","  \"\"\"\n","    Args:\n","        env: GridWorld environment\n","        dataset: tuple\n","            containing (states from, actions, rewards, states to)\n","    Return:\n","        P_hat: np.array\n","            Estimation of transition (Ns, Na, Ns)\n","        R_hat : np.array\n","            Estimation of rewards (Ns, Na)\n","  \"\"\"\n","\n","  # Get number of states and number of actions\n","  Ns, Na = env.S, env.A\n","\n","  # Initialize arrays that will be used to compute P_hat and R_hat\n","  occurenceMatrix, P_hat = np.zeros((Ns, Na, Ns)), np.zeros((Ns, Na, Ns))\n","  R_sum, R_hat = np.zeros((Ns, Na)), np.zeros((Ns, Na))\n","\n","  # For each sample of the MDP : \n","  # 1 - Count number of transition (FROM state i -> TO state j WITH action a)\n","  # 2 - Sum rewards for each transition (FROM state i WITH action a)\n","\n","  for s_i, s_j, a, r in zip(dataset[0], dataset[3], dataset[1], dataset[2]):\n","    \n","    # Compute number of times the transition from state i to j with action a\n","    occurenceMatrix[s_i, a, s_j] += 1\n","\n","    # For each (s_i, a), sum all rewards\n","    R_sum[s_i, a] += r\n","\n","\n","  # Compute number of occurences per (state i, action a)\n","  occurenceActions = np.sum(occurenceMatrix, axis=2) # Sum over action j\n","\n","  # For each state i, action a, to state j : compute P_hat and R_hat\n","\n","  for s_i in range(Ns):\n","    for a in range(Na):\n","      for s_j in range(Ns):\n","\n","        # Check if number of actions is null to not divide by 0\n","        if(occurenceActions[s_i, a] != 0):\n","\n","          # Compute P_hat(state i, action a, state j)\n","          P_hat[s_i, a, s_j] = occurenceMatrix[s_i, a, s_j]/occurenceActions[s_i, a]\n","\n","      # Compute R_hat\n","      # Check if number of actions is null to not divide by 0\n","      if(occurenceActions[s_i, a] != 0):\n","        R_hat[s_i, a] += R_sum[s_i, a]/occurenceActions[s_i, a]\n","\n","  return P_hat, R_hat"],"metadata":{"id":"QEMVOyk6lTba"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize parameters\n","gamma = 0.9\n","\n","# Compute P_hat and R_hat for dataset_1\n","P_hat1, R_hat1 = getMatrixPR(env, dataset_1)\n","P_hat2, R_hat2 = getMatrixPR(env, dataset_2)\n","\n","# Compute policy and value function for the 2 datasets\n","policy1, V1 = policy_iteration(P_hat1, R_hat1, gamma, 0)\n","policy2, V2 = policy_iteration(P_hat2, R_hat2, gamma, 0)\n","\n","# Compute true optimal policy and value function\n","policy_true, V_true = policy_iteration(env.P, env.R, gamma, 0)"],"metadata":{"id":"PSh2FTTHhGAw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compare values of true optimal value function and estimations with L1 and L-inf norm.\n","\n","# For dataset 1 (random)\n","d1_L1 = max(abs(V_true - V1))\n","d1_LInf = sum(abs(V_true - V1))\n","\n","# For dataset 2 (uniform)\n","d2_L1 = max(abs(V_true - V2))\n","d2_LInf = sum(abs(V_true - V2))\n","\n","print(f\"Error between true and estimated value functions:\\n\")\n","\n","print(f\"- Dataset 1 (random):\\nEstimated with L1 norm: {d1_L1:.2f}.\")\n","print(f\"Estimaed estimated with L-Inf norm: {d1_LInf:.2f}.\\n\")\n","\n","print(f\"- Dataset 2 (uniform):\\nEstimated with L1 norm: {d2_L1:.2f}.\")\n","print(f\"Estimated with L-Inf norm: {d2_LInf:.2f}.\")"],"metadata":{"id":"CWASGHV1lwP3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compare performance of policies\n","\n","# For dataset 1\n","print(f\"Policy of dataset 1 with {num_samples} samples (random).\")\n","render_policy(env, policy1)\n","\n","# For dataset 2\n","print(f\"\\nPolicy of dataset 2 with {num_samples} samples (uniform).\")\n","render_policy(env, policy2)\n","\n","# For true policy\n","print(f\"\\nTrue policy with {num_samples} samples.\")\n","render_policy(env, policy_true)"],"metadata":{"id":"SwNJG7DPixpW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UKINsa_yGLGL"},"source":["## Question 2.2\n","\n","Suppose that $\\hat{P}$ and $\\hat{R}$ are estimated from a dataset of exactly $N$ i.i.d. samples from **each** state-action pair. This means that, for each $(s,a)$, we have $N$ samples $\\{(s_1',r_1, \\dots, s_N', r_N\\}$, where $s_i' \\sim P(\\cdot | s,a)$ and $r_i \\sim R(s,a)$ for $i=1,\\dots,N$, and\n","$$ \\hat{P}(s'|s,a) = \\frac{1}{N}\\sum_{i=1}^N \\mathbb{1}(s_i' = s'), $$\n","$$ \\hat{R}(s,a) = \\frac{1}{N}\\sum_{i=1}^N r_i.$$\n","Suppose that $R$ is a distribution with support in $[0,1]$. Let $\\hat{V}$ be the optimal value function computed in the empirical MDP (i.e., the one with transitions $\\hat{P}$ and rewards $\\hat{R}$). For any $\\delta\\in(0,1)$, derive an upper bound to the error\n","\n","$$ \\| \\hat{V} - V^* \\|_\\infty $$\n","\n","which holds with probability at least $1-\\delta$.\n","\n","**Note** Your bound should only depend on deterministic quantities like $N$, $\\gamma$, $\\delta$, $S$, $A$. It should *not* dependent on the actual random samples.\n","\n","**Hint** The following two inequalities may be helpful.\n","\n","1. **A (simplified) lemma**. For any state $\\bar{s}$,\n","\n","$$ |\\hat{V}(\\bar{s}) - V^*(\\bar{s})| \\leq \\frac{1}{1-\\gamma}\\max_{s,a} \\left| R(s,a) - \\hat{R}(s,a) + \\gamma \\sum_{s'}(P(s'|s,a) - \\hat{P}(s'|s,a)) V^*(s') \\right|$$\n","\n","2. **Hoeffding's inequality**. Let $X_1, \\dots X_N$ be $N$ i.i.d. random variables bounded in the interval $[0,b]$ for some $b>0$. Let $\\bar{X} = \\frac{1}{N}\\sum_{i=1}^N X_i$ be the empirical mean. Then, for any $\\epsilon > 0$,\n","\n","$$ \\mathbb{P}(|\\bar{X} - \\mathbb{E}[\\bar{X}]| > \\epsilon) \\leq 2e^{-\\frac{2N\\epsilon^2}{b^2}}.$$"]},{"cell_type":"markdown","metadata":{"id":"fKmdulLaMoiN"},"source":["### **Answer**\n","\n","From the simplified Lemma number 1, it can be shown that : \n","\n","$$ \\| \\hat{V} - V^* \\|_\\infty \\leq \\frac{1}{1-\\gamma}\\max_{s,a} \\left| R(s,a) - \\hat{R}(s,a) + \\gamma \\sum_{s'}(P(s'|s,a) - \\hat{P}(s'|s,a)) V^*(s') \\right|$$\n","\n","Then, from the triangular inequality :\n","\n","$$ \\| \\hat{V} - V^* \\|_\\infty \\leq \\frac{1}{1-\\gamma}(\\max_{s,a} \\left| R(s,a) - \\hat{R}(s,a)\\right| + \\max_{s,a} \\left| \\gamma \\sum_{s'}(P(s'|s,a) - \\hat{P}(s'|s,a)) V^*(s') \\right|)$$\n","\n","In this question, it's needed to show that :  \n","\n","$$P(| \\hat{V} - V^* \\|_\\infty \\geq ϵ ) \\leq \\delta$$\n","\n","Hence, we need to consider probabilities to get the final results.\n","\n","So if we consider that :\n","\n","$$\\epsilon \\leq \\| \\hat{V} - V^* \\|_\\infty \\leq \\frac{1}{1-\\gamma}(\\max_{s,a} \\left| R(s,a) - \\hat{R}(s,a)\\right| + \\max_{s,a} \\left| \\gamma \\sum_{s'}(P(s'|s,a) - \\hat{P}(s'|s,a)) V^*(s') \\right|)$$\n","\n","Hence, we have either that :\n","\n","$$\\frac{1}{1-\\gamma}\\max_{s,a} \\left| R(s,a) - \\hat{R}(s,a)\\right| > \\frac{\\epsilon}{2}$$ or $$\\frac{\\gamma}{1-\\gamma}\\max_{s,a} \\left| \\sum_{s'}(P(s'|s,a) - \\hat{P}(s'|s,a)) V^*(s') \\right| > \\frac{\\epsilon}{2}$$\n","\n","Then, with probabilities :\n","\n","$P(|| \\hat{V} - V^* \\|_\\infty \\geq ϵ ) \\leq P(\\frac{1}{1-\\gamma}\\max_{s,a} \\left| R(s,a) - \\hat{R}(s,a)\\right| > \\frac{\\epsilon}{2}) + P(\\frac{\\gamma}{1-\\gamma}\\max_{s,a} \\left| \\sum_{s'}(P(s'|s,a) - \\hat{P}(s'|s,a)) V^*(s') \\right|) > \\frac{\\epsilon}{2})$\n","\n","Then, it comes that : \n","\n","$$P(|| \\hat{V} - V^* \\|_\\infty \\geq ϵ ) \\leq P(\\bigcup_{s, a} \\left|R(s,a) - \\hat{R}(s,a)\\right| > \\frac{\\epsilon (1-\\gamma)}{2}) + P(\\bigcup_{s, a}\\left| \\sum_{s'}(P(s'|s,a) - \\hat{P}(s'|s,a)) V^*(s') \\right| > \\frac{\\epsilon (1-\\gamma)}{2\\gamma})$$\n","\n","$$\\leq \\sum_{s, a} [ \\ P(\\left|R(s,a) - \\hat{R}(s,a)\\right| > \\frac{\\epsilon (1-\\gamma)}{2}) + P(\\left| \\sum_{s'}(P(s'|s,a) - \\hat{P}(s'|s,a)) V^*(s') \\right| > \\frac{\\epsilon (1-\\gamma)}{2\\gamma}) \\ ]$$\n","\n","<br>\n","Now, the Hoeffding's inequality will be used to find an upper bound.\n","\n","For the first term, we can have this upper bound :\n","\n","$$P(\\left|R(s,a) - \\hat{R}(s,a)\\right| > \\frac{\\epsilon (1-\\gamma)}{2}) \\leq 2e^{-\\frac{2N\\epsilon^2 (1-\\gamma)^2}{4}} = 2e^{-\\frac{N\\epsilon^2(1-\\gamma)^2}{2}}$$\n","\n","To have an upper bound for the second term, we know that $R$ is a distribution with support in $[0,1]$, and for any policy $\\pi$ :\n","\n","$$0 \\leq V^{\\pi}(s) = \\mathbb{E}[\\sum_{t}\\gamma^t R(s_t, d_t(at) | s_0=s, \\pi] \\leq \\mathbb{E}[\\sum{t}\\gamma^t | s_0=s, \\pi] = \\frac{1}{1-\\gamma}$$\n","\n","Hence,\n","\n","$$0 \\leq V^{\\pi}(s) \\leq V^{*}(s) \\leq \\frac{1}{1-\\gamma}$$\n","\n","Then :\n","\n","$$\\mathbb{E}[\\mathbb{1}(s_i' = s') V^{*}(s')] = V^{*}(s')P(s'|s,a) \\in [0, \\frac{1}{1-\\gamma}]$$\n","\n","We can now find an upper bound to the secound term :\n","\n","$$P(\\left| \\sum_{s'}(P(s'|s,a) - \\hat{P}(s'|s,a)) V^*(s') \\right| > \\frac{\\epsilon (1-\\gamma)}{2\\gamma})$$\n","$$\\leq \\sum_{s'}P(\\left| (P(s'|s,a) - \\hat{P}(s'|s,a)) V^*(s') \\right| > \\frac{\\epsilon (1-\\gamma)}{2\\gamma})$$\n","$$= \\sum_{s'}P(\\left| \\mathbb{E}[\\mathbb{1}_{s_{i}'=s'}V^{*}(s')-\\frac{1}{N}\\mathbb{1}_{s_{i}'=s'}V^{*}(s') \\right| > \\frac{\\epsilon (1-\\gamma)}{2\\gamma|S|})$$\n","$$\\leq 2|S|e^{-\\frac{N\\epsilon^2(1-\\gamma)^4}{2\\gamma^2 |S|^2}}$$\n","\n","Finally :\n","\n","$$P(|| \\hat{V} - V^* \\|_\\infty \\geq ϵ ) \\leq 2e^{-\\frac{N\\epsilon^2(1-\\gamma)^2}{2}} + 2|S|e^{-\\frac{N\\epsilon^2(1-\\gamma)^4}{2\\gamma^2 |S|^2}}$$\n","$$\\leq 4|S||A|max(e^{-\\frac{N\\epsilon^2(1-\\gamma)^2}{2}}, \\ 2e^{-\\frac{N\\epsilon^2(1-\\gamma)^4}{2\\gamma^2 |S|^2}}) $$\n","\n","And to conlude, with $\\epsilon(N, \\gamma, \\delta, S, A) = max(\\sqrt{\\frac{2}{N(1-\\gamma)^2}log(\\frac{4|A||S|}{\\delta})}, \\ \\sqrt{\\frac{2\\gamma^2|S|^2}{N(1-\\gamma)^4}log(\\frac{4|A||S|^2}{\\delta})} \\ )$ :\n","\n","$|| \\hat{V} - V^* \\|_\\infty < \\epsilon(N, \\gamma, \\delta, S, A)$, with probability at least $1-\\delta$."]},{"cell_type":"markdown","metadata":{"id":"tpqwCBG2MwxO"},"source":["## Question 2.3\n","\n","Suppose once again that we are given a dataset of $N$ samples in the form of tuples $(s_i,a_i,s_i',r_i)$. We know that each tuple contains a valid transition from the true MDP, i.e., $s_i' \\sim P(\\cdot | s_i, a_i)$ and $r_i \\sim R(s_i,a_i)$, while the state-action pairs $(s_i,a_i)$ from which the transition started can be arbitrary.\n","\n","Suppose we want to apply Q-learning to this MDP. Can you think of a way to leverage this offline data to improve the sample-efficiency of the algorithm? What if we were using SARSA instead?"]},{"cell_type":"markdown","metadata":{"id":"mbYTKetHOYU_"},"source":["### **Answer**\n","\n","For the **Q-learning algorithm** which is a model-free RL algorithm, $Q(s,a)$ is updated only when $(s,a)$ is visited. In **dynamic-Q learning**, **Q(s,a)** is updated every time $s,a$ is queried from the memory. Hence, it's not needed to revisit $s,a$ so it speeds up the algorithm. \n","\n","Therefore, **dynamic-Q learning** should be an efficient way to leverage this offline data to improve the sample-efficiency of the algorithm\n","\n","However, **SARSA** is on-policy since at each update of $Q(s,a)$, the Q-value of the next state will be used. That's why SARSA would not work since in our case, the update is made with the sample of the dataset (off-policy)."]},{"cell_type":"markdown","metadata":{"id":"542QxKsSOs21"},"source":["# Part 3 - RL with Function Approximation"]},{"cell_type":"markdown","metadata":{"id":"OiGZBiJ4PiIE"},"source":["## Question 3.1\n","\n","Given a datset $(s_i, a_i, r_i, s_i')$ of (states, actions, rewards, next states), the Fitted Q-Iteration (FQI) algorithm proceeds as follows:\n","\n","\n","* We start from a $Q$ function $Q_0 \\in \\mathcal{F}$, where $\\mathcal{F}$ is a function space;\n","* At every iteration $k$, we compute $Q_{k+1}$ as:\n","\n","$$\n","Q_{k+1}\\in\\arg\\min_{f\\in\\mathcal{F}} \\frac{1}{2}\\sum_{i=1}^N\n","\\left(\n","  f(s_i, a_i) - y_i^k\n","\\right)^2 + \\lambda \\Omega(f)\n","$$\n","where $y_i^k = r_i + \\gamma \\max_{a'}Q_k(s_i', a')$, $\\Omega(f)$ is a regularization term and $\\lambda > 0$ is the regularization coefficient.\n","\n","\n","Consider FQI with *linear* function approximation. That is, for a given feature map $\\phi : S \\rightarrow \\mathbb{R}^d$, we consider a parametric family of $Q$ functions $Q_\\theta(s,a) = \\phi(s)^T\\theta_a$ for $\\theta_a\\in\\mathbb{R}^d$. Suppose we are applying FQI on a given dataset of $N$ tuples of the form $(s_i, a_i, r_i, s_i')$ and we are at the $k$-th iteration. Let $\\theta_k \\in\\mathbb{R}^{d \\times A}$ be our current parameter. Derive the *closed-form* update to find $\\theta_{k+1}$, using $\\frac{1}{2}\\sum_a ||\\theta_a||_2^2$ as regularization."]},{"cell_type":"markdown","metadata":{"id":"7jx7aE41DkEM"},"source":["### **Answer**\n","\n","To start, we'll define : \n","\n","$$Q_\\theta(s,a) = \\phi(s)^T\\theta_a = \\phi(s)^T\\theta M_{a}$$\n","\n","where $M_{a} \\in \\mathbb{R}^A$ defined such as $\\forall i \\in [0, A] : (M_{a})_{i}=1$ if $a_{i}=a$, 0 otherwise.\n","\n","Therefore, we have the two following results :\n","\n","$$M_{a} M_{a}^{T} = I_{A}$$\n","$$\\forall i \\in \\{1, .., A\\}, \\  (M_{a}^{T} M_{a})_{i} = \\mathbb{1}_{a_i=a} $$\n","\n","-----------------\n","\n","Hence, the function to minimize becomes :\n","\n","$$ g : \\theta → \\frac{1}{2} \\sum_{i=1}^{N} (\\phi(s_i)^T\\theta M_{a_i}-y_{i}^k)^2 +\\frac{\\lambda}{2} \\sum_{a \\in A}M_a^T \\theta ^T \\theta M_a $$\n","\n","which is convex and coercive. \n","\n","-----------------\n","\n","Then, the solution of the optimization problem can be found by finding the value of the parameter $\\theta$ where the gradient of $g$ is null.\n","\n","$$\\nabla g(\\theta)=\\sum_{i=1}^N \\phi(s_i) M_{a_i}^T (\\phi(s_i)^T \\theta M_{a_i} - y_i^k) + \\lambda \\theta M_a M_a^T$$\n","$$=\\sum_{i=1}^N \\phi(s_i) M_{a_i}^T (\\phi(s_i)^T \\theta M_{a_i} - y_i^k) + \\lambda \\theta$$\n","\n","Hence, $\\theta_{k+1}$ is the value of $\\theta$ where gradient is null at this point :\n","\n","$\\nabla g(\\theta)=0 \\iff \\lambda \\theta_{k+1} +  \\sum_{i=1}^N \\phi(s_i) M_{a_i}^T \\phi(s_i)^T \\theta_{k+1} M_{a_i} = \\sum_{i=1}^N \\phi(s_i) M_{a_i}^T y_i^k$\n","\n","Then, it comes that :\n","\n","$$(\\lambda \\theta_{k+1})_{a} +  \\sum_{i=1}^N \\phi(s_i) M_{a_i}^T \\phi(s_i)^T \\theta_{k+1} M_{a_i} M_{a} = \\sum_{i=1}^N \\phi(s_i) M_{a_i}^T y_i^k M_{a}$$\n","\n","$$\\iff (\\lambda \\theta_{k+1})_{a} +  \\sum_{i=1}^N \\mathbb{1}_{a_i=a}\\phi(s_i)(\\phi(s_i)^T (\\theta_{k+1})_a) = \\sum_{i=1}^N \\mathbb{1}_{a_i=a} \\phi(s_i) y_i^k$$\n","\n","$$\\iff (\\lambda I_d +  \\sum_{i=1}^N \\mathbb{1}_{a_i=a}\\phi(s_i)\\phi(s_i)^T)(\\theta_{k+1})_{a} = \\sum_{i=1}^N \\mathbb{1}_{a_i=a} \\phi(s_i) y_i^k$$\n","\n","So finally, by matrix inversion :\n","\n","$$(\\theta_{k+1})_{a} = (\\lambda I_d +  \\sum_{i=1}^N \\mathbb{1}_{a_i=a}\\phi(s_i)\\phi(s_i)^T)^{-1} \\sum_{i=1}^N \\mathbb{1}_{a_i=a} \\phi(s_i) y_i^k$$\n"]},{"cell_type":"markdown","metadata":{"id":"ewHzjm7MVGBg"},"source":["## Question 3.2\n","\n","The code below creates a larger gridworld (with more states than the one used in the previous questions), and defines a feature map. Implement linear FQI to this environment (in the function `linear_fqi()` below), and compare the approximated $Q$ function to the optimal $Q$ function computed with value iteration.\n","\n","Can you improve the feature map in order to reduce the approximation error?"]},{"cell_type":"markdown","metadata":{"id":"Tu4g-HSnEcBs"},"source":["### **Answer**\n","\n","To improve the feature map, we can try different values of parameters of the function GridWorldFeatureMap : (dim: Feature dimension, sigma: RBF kernel bandwidth).\n","\n","With default parameters $(15, 0.25)$, the agent doesn't achieve the terminal state.\n","\n","By increasing the number of samples and changing values of parameters to $(200, 0.15)$, the results is better.\n","\n","Moreover, dataset was generated with uniform samples.\n","\n","Hence yes, the feature map can be improved in order to reduce the approximation error."]},{"cell_type":"code","metadata":{"id":"ZovF3VXOVfCs"},"source":["def get_large_gridworld():\n","  \"\"\"Creates an instance of a grid-world MDP with more states.\"\"\"\n","  walls = [(ii, 10) for ii in range(15) if (ii != 7 and ii != 8)]\n","  env = GridWorld(\n","      nrows=15,\n","      ncols=15,\n","      reward_at = {(14, 14):1.0},\n","      walls=tuple(walls),\n","      success_probability=0.9,\n","      terminal_states=((14, 14),)\n","  )\n","  return env\n","\n","\n","class GridWorldFeatureMap:\n","  \"\"\"Create features for state-action pairs\n","  \n","  Args:\n","    dim: int\n","      Feature dimension\n","    sigma: float\n","      RBF kernel bandwidth\n","  \"\"\"\n","  def __init__(self, env, dim=15, sigma=0.25):\n","    self.index2coord = env.index2coord\n","    self.n_states = env.Ns\n","    self.n_actions = env.Na\n","    self.dim = dim\n","    self.sigma = sigma\n","\n","    n_rows = env.nrows\n","    n_cols = env.ncols\n","\n","    # build similarity matrix\n","    sim_matrix = np.zeros((self.n_states, self.n_states))\n","    for ii in range(self.n_states):\n","        row_ii, col_ii = self.index2coord[ii]\n","        x_ii = row_ii / n_rows\n","        y_ii = col_ii / n_cols\n","        for jj in range(self.n_states):\n","            row_jj, col_jj = self.index2coord[jj]\n","            x_jj = row_jj / n_rows\n","            y_jj = col_jj / n_cols\n","            dist = np.sqrt((x_jj - x_ii) ** 2.0 + (y_jj - y_ii) ** 2.0)\n","            sim_matrix[ii, jj] = np.exp(-(dist / sigma) ** 2.0)\n","\n","    # factorize similarity matrix to obtain features\n","    uu, ss, vh = np.linalg.svd(sim_matrix, hermitian=True)\n","    self.feats = vh[:dim, :]\n","\n","  def map(self, observation):\n","    feat = self.feats[:, observation].copy()\n","    return feat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"InCfu7F9-TbS"},"source":["env = get_large_gridworld()\n","feat_map = GridWorldFeatureMap(env)\n","\n","# Visualize large gridworld\n","render_policy(env)\n","\n","# The features have dimension (feature_dim).\n","feature_example = feat_map.map(1) # feature representation of s=1\n","print(feature_example)\n","\n","# Initial vector theta representing the Q function\n","theta = np.zeros((feat_map.dim, env.action_space.n))\n","print(theta.shape)\n","print(feature_example @ theta) # approximation of Q(s=1, a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p21KMmruugO1"},"source":["def linear_fqi(env, feat_map, num_iterations, lambd=0.1, gamma=0.95):\n","  \"\"\"\n","  # Linear FQI implementation\n","  # TO BE COMPLETED\n","  \"\"\"\n","\n","  # get a dataset\n","  #    dataset = get_uniform_dataset(env, n_samples=...)\n","  # OR dataset = get_random_policy_dataset(env, n_samples=...)\n","\n","\n","  theta = np.zeros((feat_map.dim, env.Na))\n","\n","  # Get dataset with uniform samples\n","  (states, actions, rewards, next_states) = get_uniform_dataset(env, n_samples = 2000) \n","\n","  # For each iteration of the algorithm\n","  for it in range(num_iterations):\n","    \n","    # Compute Q at iteration k (formula above) thanks to next_states and previous theta value\n","    Q = feat_map.map(next_states).T @ theta\n","\n","    # Compute Y at iteration k thanks to Q (formula above)\n","    Y = gamma * np.max(Q, axis = 1) + rewards\n","\n","    # For action possible, update theta[:, current_action]\n","    for current_action in np.unique(actions): \n","      \n","      # Get index of current_action \n","      current_index = np.array((actions == current_action), dtype = int) \n","      # Update theta\n","      theta[:, current_action]  = np.linalg.inv(lambd*np.eye(feat_map.dim) + (feat_map.map(states)*current_index)@feat_map.map(states).T)@(feat_map.map(states)@(Y *current_index)).T\n","  \n","  return theta\n","\n","# ----------------------------\n","# Environment and feature map\n","# ----------------------------\n","env = get_large_gridworld()\n","# you can change the parameters of the feature map, and even try other maps!\n","feat_map = GridWorldFeatureMap(env, dim=200, sigma=0.15)\n","\n","# -------\n","# Run FQI\n","# -------\n","theta = linear_fqi(env, feat_map, num_iterations=100)\n","\n","# Compute and run greedy policy\n","Q_fqi = np.zeros((env.Ns, env.Na))\n","for ss in range(env.Ns):\n","  state_feat = feat_map.map(ss)\n","  Q_fqi[ss, :] = state_feat @ theta\n","\n","V_fqi = Q_fqi.max(axis=1)\n","policy_fqi = Q_fqi.argmax(axis=1)\n","render_policy(env, policy_fqi, horizon=100)\n","\n","# Visualize the approximate value function in the gridworld.\n","img = env.get_layout_img(V_fqi)    \n","plt.imshow(img)\n","plt.title(\"Approximate value function in the gridworld\")\n","plt.show()"],"execution_count":null,"outputs":[]}]}